{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44f0940-2047-4e73-abda-4a9e2dbb2d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURA√á√ïES E IMPORTS\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "# Para o XGBoost, a instala√ß√£o e importa√ß√£o pode variar no Databricks\n",
    "# Vamos usar o GBTClassifier como um substituto poderoso e nativo do Spark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar o uso do nosso cat√°logo e dos schemas\n",
    "spark.sql(\"USE CATALOG previsao_brasileirao\")\n",
    "gold_schema = \"gold\"\n",
    "diamond_schema = \"diamond\"\n",
    "\n",
    "print(f\"Lendo dados de: {gold_schema}\")\n",
    "print(f\"Salvando modelos e m√©tricas em: {diamond_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d557b8cf-500a-4708-b754-30451951897d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 1: CARREGAR A FEATURE STORE E DIVIDIR EM TREINO/TESTE\n",
    "# ==============================================================================\n",
    "print(\"Carregando feature_store e dividindo os dados...\")\n",
    "\n",
    "# Carregar a tabela final da camada Gold\n",
    "df_features = spark.table(f\"{gold_schema}.feature_store\")\n",
    "\n",
    "# Divis√£o aleat√≥ria dos dados\n",
    "(df_treino, df_teste) = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Total de partidas: {df_features.count()}\")\n",
    "print(f\"Partidas para Treino (80%): {df_treino.count()}\")\n",
    "print(f\"Partidas para Teste (20%): {df_teste.count()}\")\n",
    "\n",
    "\n",
    "# --- AJUSTE APLICADO: QUEBRANDO A LINHAGEM DO DATAFRAME DE FORMA DEFINITIVA ---\n",
    "\n",
    "# 1. Obter os labels distintos como um DataFrame Spark\n",
    "df_labels_spark = df_features.select(\"resultado\").distinct()\n",
    "\n",
    "# 2. Coletar os resultados para o driver como uma lista Python.\n",
    "#    Isso tira os dados do contexto do Spark e quebra qualquer linhagem.\n",
    "#    A a√ß√£o .collect() aqui √© segura, pois estamos coletando apenas 3 strings.\n",
    "labels_python_list = [row['resultado'] for row in df_labels_spark.collect()]\n",
    "\n",
    "# 3. Criar um DataFrame Spark completamente novo e limpo a partir da lista Python.\n",
    "#    Este DataFrame n√£o tem nenhum plano de execu√ß√£o complexo associado.\n",
    "df_labels_limpo = spark.createDataFrame(labels_python_list, \"string\").withColumnRenamed(\"value\", \"resultado\")\n",
    "\n",
    "# 4. Treinar (fit) o StringIndexer SOMENTE neste DataFrame limpo.\n",
    "label_indexer = StringIndexer(inputCol=\"resultado\", outputCol=\"label\")\n",
    "label_indexer_model = label_indexer.fit(df_labels_limpo)\n",
    "\n",
    "# 5. Usar o modelo J√Å TREINADO para transformar os DataFrames de treino e teste.\n",
    "df_treino = label_indexer_model.transform(df_treino)\n",
    "df_teste = label_indexer_model.transform(df_teste)\n",
    "\n",
    "# --- FIM DO AJUSTE ---\n",
    "\n",
    "print(\"\\nAmostra do DataFrame de Treino:\")\n",
    "display(df_treino.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc178f04-5b60-47e2-81a0-e1dd5b6c647a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 2: TREINAR OS MODELOS ESPECIALISTAS (N√çVEL 1)\n",
    "# ==============================================================================\n",
    "print(\"Treinando os 3 modelos especialistas do N√≠vel 1...\")\n",
    "\n",
    "# --- Modelo A: O Generalista (RandomForestClassifier) ---\n",
    "# Usa todas as features dispon√≠veis\n",
    "features_gerais_nomes = [col for col in df_treino.columns if col not in [\"partida_id\", \"rodada\", \"mandante_id\", \"visitante_id\", \"resultado\", \"label\"]]\n",
    "\n",
    "# Converter todas as features para DoubleType para garantir consist√™ncia\n",
    "df_treino_geral = df_treino.select(\n",
    "    \"label\",\n",
    "    *[F.col(c).cast(\"double\").alias(c) for c in features_gerais_nomes]\n",
    ")\n",
    "\n",
    "va_geral = VectorAssembler(inputCols=features_gerais_nomes, outputCol=\"features\")\n",
    "\n",
    "# AJUSTE: Trocado GBTClassifier por RandomForestClassifier, que suporta multiclass\n",
    "rf_geral = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100, maxDepth=5)\n",
    "\n",
    "pipeline_geral = Pipeline(stages=[va_geral, rf_geral])\n",
    "modelo_geral = pipeline_geral.fit(df_treino_geral)\n",
    "print(\"  - ‚úÖ Modelo A (Generalista - RandomForest) treinado.\")\n",
    "\n",
    "\n",
    "# --- Modelo B: O Estat√≠stico (Regress√£o Log√≠stica) ---\n",
    "# Este modelo j√° suporta multiclass nativamente\n",
    "features_estatisticas_nomes = [\"elo_mandante_pre_jogo\", \"elo_visitante_pre_jogo\", \"elo_diff\", \"mm_gols_m\", \"mm_gols_v\", \"diff_mm_gols\"]\n",
    "\n",
    "df_treino_lr = df_treino.select(\n",
    "    \"label\",\n",
    "    *[F.col(c).cast(\"double\").alias(c) for c in features_estatisticas_nomes]\n",
    ")\n",
    "\n",
    "va_estatistico = VectorAssembler(inputCols=features_estatisticas_nomes, outputCol=\"features\")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "pipeline_lr = Pipeline(stages=[va_estatistico, lr])\n",
    "modelo_lr = pipeline_lr.fit(df_treino_lr)\n",
    "print(\"  - ‚úÖ Modelo B (Estat√≠stico - Regress√£o Log√≠stica) treinado.\")\n",
    "\n",
    "\n",
    "# --- Modelo C: O T√°tico (Random Forest) ---\n",
    "# Este modelo tamb√©m j√° suporta multiclass nativamente\n",
    "features_taticas_nomes = [\"diff_mm_fin\", \"diff_mm_des\"]\n",
    "\n",
    "df_treino_rf = df_treino.select(\n",
    "    \"label\",\n",
    "    *[F.col(c).cast(\"double\").alias(c) for c in features_taticas_nomes]\n",
    ")\n",
    "\n",
    "va_tatico = VectorAssembler(inputCols=features_taticas_nomes, outputCol=\"features\")\n",
    "rf_tatico = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, maxDepth=5)\n",
    "pipeline_rf = Pipeline(stages=[va_tatico, rf_tatico])\n",
    "modelo_rf = pipeline_rf.fit(df_treino_rf)\n",
    "print(\"  - ‚úÖ Modelo C (T√°tico - RandomForest) treinado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f097dd6-66b8-4978-b419-13b1ad88393f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 3: GERAR PREVIS√ïES E AVALIAR CADA ESPECIALISTA INDIVIDUALMENTE\n",
    "# ==============================================================================\n",
    "print(\"Gerando previs√µes dos especialistas e avaliando cada um...\")\n",
    "\n",
    "# Obter as previs√µes de cada modelo especialista\n",
    "pred_geral = modelo_geral.transform(df_teste) \n",
    "pred_lr = modelo_lr.transform(df_teste)\n",
    "pred_rf = modelo_rf.transform(df_teste)\n",
    "\n",
    "# --- AVALIA√á√ÉO INDIVIDUAL ---\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "acc_geral = evaluator.evaluate(pred_geral)\n",
    "acc_lr = evaluator.evaluate(pred_lr)\n",
    "acc_rf = evaluator.evaluate(pred_rf)\n",
    "\n",
    "print(\"\\n--- Acur√°cia dos Modelos Especialistas (N√≠vel 1) ---\")\n",
    "print(f\"  - Modelo A (Generalista - RandomForest): {acc_geral:.2%}\")\n",
    "print(f\"  - Modelo B (Estat√≠stico - Regress√£o Log.): {acc_lr:.2%}\")\n",
    "print(f\"  - Modelo C (T√°tico - RandomForest): {acc_rf:.2%}\")\n",
    "\n",
    "\n",
    "# --- PREPARA√á√ÉO PARA O META-MODELO ---\n",
    "# Renomear as colunas de probabilidade para evitar conflitos\n",
    "meta_features_geral = pred_geral.select(\"partida_id\", \"label\", F.col(\"probability\").alias(\"prob_geral\"))\n",
    "meta_features_lr = pred_lr.select(\"partida_id\", F.col(\"probability\").alias(\"prob_lr\"))\n",
    "meta_features_rf = pred_rf.select(\"partida_id\", F.col(\"probability\").alias(\"prob_rf\"))\n",
    "\n",
    "# Juntar as previs√µes para formar o DataFrame de treino do N√≠vel 2\n",
    "df_treino_meta = meta_features_geral.join(meta_features_lr, \"partida_id\").join(meta_features_rf, \"partida_id\")\n",
    "\n",
    "# Criar um √∫nico vetor de features para o meta-modelo\n",
    "assembler_meta = VectorAssembler(inputCols=[\"prob_geral\", \"prob_lr\", \"prob_rf\"], outputCol=\"features\")\n",
    "df_treino_meta = assembler_meta.transform(df_treino_meta)\n",
    "\n",
    "print(\"\\n‚úÖ Meta-features criadas com sucesso para o N√≠vel 2.\")\n",
    "display(df_treino_meta.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97de204-6de7-456c-9e44-a4e0da1ee5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 4: TREINAR, AVALIAR E SALVAR OS RESULTADOS DO META-MODELO (N√çVEL 2)\n",
    "# ==============================================================================\n",
    "print(\"Treinando e avaliando o meta-modelo final...\")\n",
    "\n",
    "# Usaremos uma Regress√£o Log√≠stica como o \"blender\" final.\n",
    "meta_modelo = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "modelo_final = meta_modelo.fit(df_treino_meta)\n",
    "\n",
    "# Fazer as previs√µes finais no mesmo dataset de teste\n",
    "previsoes_finais = modelo_final.transform(df_treino_meta)\n",
    "\n",
    "# Avaliar a performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracia = evaluator.evaluate(previsoes_finais)\n",
    "\n",
    "print(f\"\\n--- AVALIA√á√ÉO FINAL DO MODELO H√çBRIDO ---\")\n",
    "print(f\"üéØ Acur√°cia nas √∫ltimas 5 rodadas: {acuracia:.2%}\")\n",
    "\n",
    "# Exibir a Matriz de Confus√£o para uma an√°lise mais detalhada\n",
    "print(\"\\nMatriz de Confus√£o:\")\n",
    "display(previsoes_finais.groupBy(\"label\", \"prediction\").count())\n",
    "\n",
    "\n",
    "# --- AJUSTE APLICADO AQUI: PERSISTINDO OS RESULTADOS NA CAMADA DIAMOND ---\n",
    "\n",
    "# Selecionar as colunas mais importantes para a an√°lise de performance\n",
    "df_resultados_diamond = previsoes_finais.select(\n",
    "    \"partida_id\",\n",
    "    \"label\",         # O resultado real (0=V_MAND, 1=EMP, 2=V_VIS)\n",
    "    \"prediction\",    # A previs√£o do nosso modelo\n",
    "    \"probability\"    # A confian√ßa do modelo em cada uma das 3 classes\n",
    ")\n",
    "\n",
    "# Salvar esta tabela de resultados na Camada Diamond\n",
    "df_resultados_diamond.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{diamond_schema}.previsoes_validadas\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tabela de previs√µes e resultados salva com sucesso em '{diamond_schema}.previsoes_validadas'\")\n",
    "\n",
    "# Em um cen√°rio de produ√ß√£o, voc√™ tamb√©m salvaria o objeto do modelo treinado,\n",
    "# geralmente usando MLflow, que se integra perfeitamente ao Databricks.\n",
    "# mlflow.spark.log_model(modelo_final, \"modelo_previsao_brasileirao\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Diamond_Model_Training_and_Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
