{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44f0940-2047-4e73-abda-4a9e2dbb2d70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CONFIGURA√á√ïES E IMPORTS\n",
    "# ==============================================================================\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "# Para o XGBoost, a instala√ß√£o e importa√ß√£o pode variar no Databricks\n",
    "# Vamos usar o GBTClassifier como um substituto poderoso e nativo do Spark\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "# Configurar o uso do nosso cat√°logo e dos schemas\n",
    "spark.sql(\"USE CATALOG previsao_brasileirao\")\n",
    "gold_schema = \"gold\"\n",
    "diamond_schema = \"diamond\"\n",
    "\n",
    "print(f\"Lendo dados de: {gold_schema}\")\n",
    "print(f\"Salvando modelos e m√©tricas em: {diamond_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d557b8cf-500a-4708-b754-30451951897d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 1: CARREGAR A FEATURE STORE E DIVIDIR EM TREINO/TESTE\n",
    "# ==============================================================================\n",
    "print(\"Carregando feature_store e dividindo os dados...\")\n",
    "\n",
    "# Carregar a tabela final da camada Gold\n",
    "df_features = spark.table(f\"{gold_schema}.feature_store\")\n",
    "\n",
    "# Divis√£o aleat√≥ria dos dados\n",
    "(df_treino, df_teste) = df_features.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Total de partidas: {df_features.count()}\")\n",
    "print(f\"Partidas para Treino (80%): {df_treino.count()}\")\n",
    "print(f\"Partidas para Teste (20%): {df_teste.count()}\")\n",
    "\n",
    "\n",
    "# --- AJUSTE DEFINITIVO: INDEXA√á√ÉO MANUAL VIA JOIN PARA EVITAR O BUG DO .fit() ---\n",
    "\n",
    "# 1. Obter os labels distintos e atribuir um ID (√≠ndice) a cada um.\n",
    "#    A fun√ß√£o window row_number() cria os √≠ndices 0, 1, 2...\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_labels_map = df_features.select(\"resultado\").distinct() \\\n",
    "    .withColumn(\"label\", (F.row_number().over(Window.orderBy(\"resultado\")) - 1).cast(\"double\"))\n",
    "\n",
    "print(\"\\nDicion√°rio de Mapeamento (Label -> √çndice) criado:\")\n",
    "display(df_labels_map)\n",
    "\n",
    "# 2. Usar um JOIN para adicionar a coluna 'label' aos DataFrames de treino e teste.\n",
    "#    Esta opera√ß√£o √© fundamental no Spark e n√£o sofre do bug de tamanho.\n",
    "df_treino = df_treino.join(df_labels_map, on=\"resultado\", how=\"inner\")\n",
    "df_teste = df_teste.join(df_labels_map, on=\"resultado\", how=\"inner\")\n",
    "\n",
    "# --- FIM DO AJUSTE ---\n",
    "\n",
    "print(\"\\nAmostra do DataFrame de Treino (agora com a coluna 'label' adicionada via join):\")\n",
    "display(df_treino.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc178f04-5b60-47e2-81a0-e1dd5b6c647a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 2: TREINAR OS MODELOS ESPECIALISTAS (N√çVEL 1)\n",
    "# ==============================================================================\n",
    "print(\"Treinando os 3 modelos especialistas do N√≠vel 1...\")\n",
    "\n",
    "# --- Modelo A: O Generalista (RandomForestClassifier) ---\n",
    "# Usa todas as features dispon√≠veis\n",
    "features_gerais_nomes = [col for col in df_treino.columns if col not in [\"partida_id\", \"rodada\", \"mandante_id\", \"visitante_id\", \"resultado\", \"label\"]]\n",
    "\n",
    "# Converter todas as features para DoubleType para garantir consist√™ncia\n",
    "df_treino_geral = df_treino.select(\n",
    "    \"label\",\n",
    "    *[F.col(c).cast(\"double\").alias(c) for c in features_gerais_nomes]\n",
    ")\n",
    "\n",
    "va_geral = VectorAssembler(inputCols=features_gerais_nomes, outputCol=\"features\")\n",
    "\n",
    "# AJUSTE: Trocado GBTClassifier por RandomForestClassifier, que suporta multiclass\n",
    "rf_geral = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=100, maxDepth=5)\n",
    "\n",
    "pipeline_geral = Pipeline(stages=[va_geral, rf_geral])\n",
    "modelo_geral = pipeline_geral.fit(df_treino_geral)\n",
    "print(\"  - ‚úÖ Modelo A (Generalista - RandomForest) treinado.\")\n",
    "\n",
    "\n",
    "# --- Modelo B: O Estat√≠stico (Regress√£o Log√≠stica) ---\n",
    "# Este modelo j√° suporta multiclass nativamente\n",
    "features_estatisticas_nomes = [\"elo_mandante_pre_jogo\", \"elo_visitante_pre_jogo\", \"elo_diff\", \"mm_gols_m\", \"mm_gols_v\", \"diff_mm_gols\"]\n",
    "\n",
    "df_treino_lr = df_treino.select(\n",
    "    \"label\",\n",
    "    *[F.col(c).cast(\"double\").alias(c) for c in features_estatisticas_nomes]\n",
    ")\n",
    "\n",
    "va_estatistico = VectorAssembler(inputCols=features_estatisticas_nomes, outputCol=\"features\")\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "pipeline_lr = Pipeline(stages=[va_estatistico, lr])\n",
    "modelo_lr = pipeline_lr.fit(df_treino_lr)\n",
    "print(\"  - ‚úÖ Modelo B (Estat√≠stico - Regress√£o Log√≠stica) treinado.\")\n",
    "\n",
    "\n",
    "# --- Modelo C: O T√°tico (Random Forest) ---\n",
    "# Este modelo tamb√©m j√° suporta multiclass nativamente\n",
    "features_taticas_nomes = [\"diff_mm_fin\", \"diff_mm_des\"]\n",
    "\n",
    "df_treino_rf = df_treino.select(\n",
    "    \"label\",\n",
    "    *[F.col(c).cast(\"double\").alias(c) for c in features_taticas_nomes]\n",
    ")\n",
    "\n",
    "va_tatico = VectorAssembler(inputCols=features_taticas_nomes, outputCol=\"features\")\n",
    "rf_tatico = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=50, maxDepth=5)\n",
    "pipeline_rf = Pipeline(stages=[va_tatico, rf_tatico])\n",
    "modelo_rf = pipeline_rf.fit(df_treino_rf)\n",
    "print(\"  - ‚úÖ Modelo C (T√°tico - RandomForest) treinado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f097dd6-66b8-4978-b419-13b1ad88393f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 3: GERAR PREVIS√ïES E AVALIAR CADA ESPECIALISTA INDIVIDUALMENTE\n",
    "# ==============================================================================\n",
    "print(\"Gerando previs√µes dos especialistas e avaliando cada um...\")\n",
    "\n",
    "# Obter as previs√µes de cada modelo especialista\n",
    "pred_geral = modelo_geral.transform(df_teste) \n",
    "pred_lr = modelo_lr.transform(df_teste)\n",
    "pred_rf = modelo_rf.transform(df_teste)\n",
    "\n",
    "# --- AVALIA√á√ÉO INDIVIDUAL ---\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "acc_geral = evaluator.evaluate(pred_geral)\n",
    "acc_lr = evaluator.evaluate(pred_lr)\n",
    "acc_rf = evaluator.evaluate(pred_rf)\n",
    "\n",
    "print(\"\\n--- Acur√°cia dos Modelos Especialistas (N√≠vel 1) ---\")\n",
    "print(f\"  - Modelo A (Generalista - RandomForest): {acc_geral:.2%}\")\n",
    "print(f\"  - Modelo B (Estat√≠stico - Regress√£o Log.): {acc_lr:.2%}\")\n",
    "print(f\"  - Modelo C (T√°tico - RandomForest): {acc_rf:.2%}\")\n",
    "\n",
    "\n",
    "# --- PREPARA√á√ÉO PARA O META-MODELO ---\n",
    "# Renomear as colunas de probabilidade para evitar conflitos\n",
    "meta_features_geral = pred_geral.select(\"partida_id\", \"label\", F.col(\"probability\").alias(\"prob_geral\"))\n",
    "meta_features_lr = pred_lr.select(\"partida_id\", F.col(\"probability\").alias(\"prob_lr\"))\n",
    "meta_features_rf = pred_rf.select(\"partida_id\", F.col(\"probability\").alias(\"prob_rf\"))\n",
    "\n",
    "# Juntar as previs√µes para formar o DataFrame de treino do N√≠vel 2\n",
    "df_treino_meta = meta_features_geral.join(meta_features_lr, \"partida_id\").join(meta_features_rf, \"partida_id\")\n",
    "\n",
    "# Criar um √∫nico vetor de features para o meta-modelo\n",
    "assembler_meta = VectorAssembler(inputCols=[\"prob_geral\", \"prob_lr\", \"prob_rf\"], outputCol=\"features\")\n",
    "df_treino_meta = assembler_meta.transform(df_treino_meta)\n",
    "\n",
    "print(\"\\n‚úÖ Meta-features criadas com sucesso para o N√≠vel 2.\")\n",
    "display(df_treino_meta.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f97de204-6de7-456c-9e44-a4e0da1ee5e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 4: TREINAR, AVALIAR E SALVAR OS RESULTADOS DO META-MODELO (N√çVEL 2)\n",
    "# ==============================================================================\n",
    "print(\"Treinando e avaliando o meta-modelo final...\")\n",
    "\n",
    "# Usaremos uma Regress√£o Log√≠stica como o \"blender\" final.\n",
    "meta_modelo = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20)\n",
    "modelo_final = meta_modelo.fit(df_treino_meta)\n",
    "\n",
    "# Fazer as previs√µes finais no mesmo dataset de teste\n",
    "previsoes_finais = modelo_final.transform(df_treino_meta)\n",
    "\n",
    "# Avaliar a performance\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "acuracia = evaluator.evaluate(previsoes_finais)\n",
    "\n",
    "print(f\"\\n--- AVALIA√á√ÉO FINAL DO MODELO H√çBRIDO ---\")\n",
    "print(f\"üéØ Acur√°cia nas √∫ltimas 5 rodadas: {acuracia:.2%}\")\n",
    "\n",
    "# Exibir a Matriz de Confus√£o para uma an√°lise mais detalhada\n",
    "print(\"\\nMatriz de Confus√£o:\")\n",
    "display(previsoes_finais.groupBy(\"label\", \"prediction\").count())\n",
    "\n",
    "\n",
    "# --- AJUSTE APLICADO AQUI: PERSISTINDO OS RESULTADOS NA CAMADA DIAMOND ---\n",
    "\n",
    "# Selecionar as colunas mais importantes para a an√°lise de performance\n",
    "df_resultados_diamond = previsoes_finais.select(\n",
    "    \"partida_id\",\n",
    "    \"label\",         # O resultado real (0=V_MAND, 1=EMP, 2=V_VIS)\n",
    "    \"prediction\",    # A previs√£o do nosso modelo\n",
    "    \"probability\"    # A confian√ßa do modelo em cada uma das 3 classes\n",
    ")\n",
    "\n",
    "# Salvar esta tabela de resultados na Camada Diamond\n",
    "df_resultados_diamond.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{diamond_schema}.previsoes_validadas\")\n",
    "\n",
    "print(f\"\\n‚úÖ Tabela de previs√µes e resultados salva com sucesso em '{diamond_schema}.previsoes_validadas'\")\n",
    "\n",
    "# Em um cen√°rio de produ√ß√£o, voc√™ tamb√©m salvaria o objeto do modelo treinado,\n",
    "# geralmente usando MLflow, que se integra perfeitamente ao Databricks.\n",
    "# mlflow.spark.log_model(modelo_final, \"modelo_previsao_brasileirao\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6922909d-acca-4fcb-907c-f4937c24927e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# VERIFICA√á√ÉO PR√â-EXECU√á√ÉO: Execute ANTES do Passo 5\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERIFICA√á√ÉO PR√â-EXECU√á√ÉO DO PASSO 5\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Lista de vari√°veis que DEVEM existir\n",
    "variaveis_necessarias = {\n",
    "    'modelo_geral': 'Modelo Generalista (RandomForest)',\n",
    "    'modelo_lr': 'Modelo Estat√≠stico (Logistic Regression)',\n",
    "    'modelo_rf': 'Modelo T√°tico (RandomForest)',\n",
    "    'modelo_final': 'Meta-Modelo (Logistic Regression)',\n",
    "    'df_labels_map': 'Mapeamento de Labels',\n",
    "    'acc_geral': 'Acur√°cia do Modelo Geral',\n",
    "    'acc_lr': 'Acur√°cia do Modelo LR',\n",
    "    'acc_rf': 'Acur√°cia do Modelo RF',\n",
    "    'acuracia': 'Acur√°cia do Meta-Modelo',\n",
    "    'df_treino': 'DataFrame de Treino',\n",
    "    'df_teste': 'DataFrame de Teste'\n",
    "}\n",
    "\n",
    "print(\"\\nüîç Verificando vari√°veis necess√°rias...\\n\")\n",
    "\n",
    "todas_ok = True\n",
    "variaveis_faltando = []\n",
    "\n",
    "for var_nome, descricao in variaveis_necessarias.items():\n",
    "    if var_nome in dir():\n",
    "        # Verificar o tipo da vari√°vel\n",
    "        var_obj = eval(var_nome)\n",
    "        tipo = type(var_obj).__name__\n",
    "        \n",
    "        # Para DataFrames, mostrar a contagem\n",
    "        if 'df_' in var_nome:\n",
    "            try:\n",
    "                count = var_obj.count()\n",
    "                print(f\"‚úÖ {var_nome:<20} ({descricao}) - {count:,} registros\")\n",
    "            except:\n",
    "                print(f\"‚úÖ {var_nome:<20} ({descricao})\")\n",
    "        # Para m√©tricas, mostrar o valor\n",
    "        elif 'acc' in var_nome or var_nome == 'acuracia':\n",
    "            print(f\"‚úÖ {var_nome:<20} ({descricao}) - {var_obj:.2%}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {var_nome:<20} ({descricao})\")\n",
    "    else:\n",
    "        print(f\"‚ùå {var_nome:<20} ({descricao}) - N√ÉO ENCONTRADA\")\n",
    "        todas_ok = False\n",
    "        variaveis_faltando.append(var_nome)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "if todas_ok:\n",
    "    print(\"‚úÖ TUDO PRONTO! Voc√™ pode executar o Passo 5.\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"‚ùå ATEN√á√ÉO! Algumas vari√°veis est√£o faltando.\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nVari√°veis faltando: {', '.join(variaveis_faltando)}\")\n",
    "    print(\"\\nüîß SOLU√á√ÉO:\")\n",
    "    print(\"1. N√ÉO execute o Passo 5 ainda\")\n",
    "    print(\"2. Volte e execute TODOS os passos anteriores:\")\n",
    "    print(\"   - PASSO 1: Configura√ß√µes\")\n",
    "    print(\"   - PASSO 2: Carregar e dividir dados\")\n",
    "    print(\"   - PASSO 3: Treinar modelos especialistas\")\n",
    "    print(\"   - PASSO 4: Gerar previs√µes e avaliar\")\n",
    "    print(\"   - (PASSO 4 continua√ß√£o): Treinar meta-modelo\")\n",
    "    print(\"3. Aguarde CADA passo terminar completamente\")\n",
    "    print(\"4. Execute esta verifica√ß√£o novamente\")\n",
    "    print(\"5. S√≥ execute o Passo 5 quando tudo estiver ‚úÖ\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Informa√ß√µes adicionais √∫teis\n",
    "print(\"\\n‚ÑπÔ∏è INFORMA√á√ïES DO AMBIENTE:\")\n",
    "print(f\"   Cat√°logo atual: {spark.catalog.currentCatalog()}\")\n",
    "print(f\"   Schema atual: {spark.catalog.currentDatabase()}\")\n",
    "\n",
    "# Verificar se o schema diamond existe\n",
    "try:\n",
    "    tabelas = spark.sql(\"SHOW TABLES IN previsao_brasileirao.diamond\").collect()\n",
    "    print(f\"   Tabelas em 'diamond': {len(tabelas)}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Schema 'diamond' pode n√£o existir: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32800536-399e-4ca4-90e0-0cb744dcf1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# PASSO 5: SALVAR MODELOS (COM SUPORTE A VOLUMES)\n",
    "# ==============================================================================\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SALVANDO MODELOS - DATABRICKS SERVERLESS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "schema_destino = \"diamond\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PASSO 1: CRIAR/VERIFICAR VOLUME PARA MODELOS\n",
    "# ==============================================================================\n",
    "print(\"\\n[Configura√ß√£o] Criando estrutura de Volumes...\")\n",
    "\n",
    "# Criar o volume se n√£o existir\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE VOLUME IF NOT EXISTS previsao_brasileirao.{schema_destino}.mlflow_models\n",
    "        COMMENT 'Volume para armazenamento tempor√°rio de modelos MLflow'\n",
    "    \"\"\")\n",
    "    print(\"  ‚úÖ Volume 'mlflow_models' configurado\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ÑπÔ∏è Volume j√° existe ou erro: {e}\")\n",
    "\n",
    "# Definir o caminho do volume\n",
    "volume_path = f\"/Volumes/previsao_brasileirao/{schema_destino}/mlflow_models\"\n",
    "print(f\"  üìÅ Caminho do volume: {volume_path}\")\n",
    "\n",
    "# Configurar vari√°vel de ambiente (alternativa 1)\n",
    "os.environ['MLFLOW_DFS_TMP'] = volume_path\n",
    "\n",
    "# ==============================================================================\n",
    "# PASSO 2: CONFIGURAR MLFLOW\n",
    "# ==============================================================================\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "\n",
    "# Encerrar run anterior se existir\n",
    "try:\n",
    "    mlflow.end_run()\n",
    "    print(\"  ‚ÑπÔ∏è Run anterior encerrada\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Configurar experimento\n",
    "experiment_name = \"/Shared/previsao_brasileirao\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "print(f\"  üìù Experimento: {experiment_name}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PASSO 3: SALVAR MODELOS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INICIANDO SALVAMENTO DOS MODELOS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with mlflow.start_run(run_name=f\"treinamento_{datetime.now().strftime('%Y%m%d_%H%M%S')}\") as run:\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"\\nüÜî Run ID: {run_id}\")\n",
    "    print(f\"üìù COPIE E GUARDE ESTE ID!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ========== SALVAR MODELO 1: GERAL ==========\n",
    "    print(\"\\n[1/4] Salvando Modelo Geral...\")\n",
    "    try:\n",
    "        mlflow.spark.log_model(\n",
    "            spark_model=modelo_geral,\n",
    "            artifact_path=\"modelo_geral\",\n",
    "            dfs_tmpdir=volume_path  # Especificar volume explicitamente\n",
    "        )\n",
    "        print(\"  ‚úÖ Modelo Geral salvo!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # ========== SALVAR MODELO 2: LR ==========\n",
    "    print(\"\\n[2/4] Salvando Modelo LR...\")\n",
    "    try:\n",
    "        mlflow.spark.log_model(\n",
    "            spark_model=modelo_lr,\n",
    "            artifact_path=\"modelo_lr\",\n",
    "            dfs_tmpdir=volume_path\n",
    "        )\n",
    "        print(\"  ‚úÖ Modelo LR salvo!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # ========== SALVAR MODELO 3: RF ==========\n",
    "    print(\"\\n[3/4] Salvando Modelo RF...\")\n",
    "    try:\n",
    "        mlflow.spark.log_model(\n",
    "            spark_model=modelo_rf,\n",
    "            artifact_path=\"modelo_rf\",\n",
    "            dfs_tmpdir=volume_path\n",
    "        )\n",
    "        print(\"  ‚úÖ Modelo RF salvo!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # ========== SALVAR MODELO 4: FINAL ==========\n",
    "    print(\"\\n[4/4] Salvando Meta-Modelo...\")\n",
    "    try:\n",
    "        mlflow.spark.log_model(\n",
    "            spark_model=modelo_final,\n",
    "            artifact_path=\"modelo_final\",\n",
    "            dfs_tmpdir=volume_path\n",
    "        )\n",
    "        print(\"  ‚úÖ Meta-Modelo salvo!\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Erro: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # ========== SALVAR M√âTRICAS ==========\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Salvando m√©tricas...\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    mlflow.log_metric(\"acuracia_geral\", float(acc_geral))\n",
    "    mlflow.log_metric(\"acuracia_lr\", float(acc_lr))\n",
    "    mlflow.log_metric(\"acuracia_rf\", float(acc_rf))\n",
    "    mlflow.log_metric(\"acuracia_final\", float(acuracia))\n",
    "    \n",
    "    mlflow.log_param(\"num_treino\", df_treino.count())\n",
    "    mlflow.log_param(\"num_teste\", df_teste.count())\n",
    "    mlflow.log_param(\"data_treinamento\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "    mlflow.log_param(\"volume_path\", volume_path)\n",
    "    \n",
    "    print(\"  ‚úÖ M√©tricas e par√¢metros salvos!\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úÖ TODOS OS MODELOS SALVOS COM SUCESSO!\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# ==============================================================================\n",
    "# PASSO 4: CRIAR TABELA DE METADADOS\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Criando tabela de metadados...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "modelos_info = spark.createDataFrame([\n",
    "    {\n",
    "        \"nome_modelo\": \"modelo_geral\",\n",
    "        \"tipo\": \"RandomForestClassifier\",\n",
    "        \"features\": \"todas\",\n",
    "        \"acuracia\": float(acc_geral),\n",
    "        \"run_id\": run_id,\n",
    "        \"caminho_mlflow\": f\"runs:/{run_id}/modelo_geral\",\n",
    "        \"volume_path\": volume_path,\n",
    "        \"data_registro\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"ativo\": True,\n",
    "        \"versao\": 1\n",
    "    },\n",
    "    {\n",
    "        \"nome_modelo\": \"modelo_lr\",\n",
    "        \"tipo\": \"LogisticRegression\",\n",
    "        \"features\": \"estatisticas\",\n",
    "        \"acuracia\": float(acc_lr),\n",
    "        \"run_id\": run_id,\n",
    "        \"caminho_mlflow\": f\"runs:/{run_id}/modelo_lr\",\n",
    "        \"volume_path\": volume_path,\n",
    "        \"data_registro\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"ativo\": True,\n",
    "        \"versao\": 1\n",
    "    },\n",
    "    {\n",
    "        \"nome_modelo\": \"modelo_rf\",\n",
    "        \"tipo\": \"RandomForestClassifier\",\n",
    "        \"features\": \"taticas\",\n",
    "        \"acuracia\": float(acc_rf),\n",
    "        \"run_id\": run_id,\n",
    "        \"caminho_mlflow\": f\"runs:/{run_id}/modelo_rf\",\n",
    "        \"volume_path\": volume_path,\n",
    "        \"data_registro\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"ativo\": True,\n",
    "        \"versao\": 1\n",
    "    },\n",
    "    {\n",
    "        \"nome_modelo\": \"modelo_final\",\n",
    "        \"tipo\": \"LogisticRegression (Meta)\",\n",
    "        \"features\": \"meta_features\",\n",
    "        \"acuracia\": float(acuracia),\n",
    "        \"run_id\": run_id,\n",
    "        \"caminho_mlflow\": f\"runs:/{run_id}/modelo_final\",\n",
    "        \"volume_path\": volume_path,\n",
    "        \"data_registro\": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        \"ativo\": True,\n",
    "        \"versao\": 1\n",
    "    }\n",
    "])\n",
    "\n",
    "# Salvar tabela\n",
    "modelos_info.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(f\"{schema_destino}.modelos_registry\")\n",
    "\n",
    "print(f\"  ‚úÖ Tabela '{schema_destino}.modelos_registry' criada!\")\n",
    "\n",
    "# Atualizar label mapping\n",
    "df_labels_map.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(f\"{schema_destino}.label_mapping\")\n",
    "\n",
    "print(f\"  ‚úÖ Tabela '{schema_destino}.label_mapping' atualizada!\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PASSO 5: VERIFICA√á√ÉO FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VERIFICA√á√ÉO FINAL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä Tabela de Metadados dos Modelos:\")\n",
    "display(spark.table(f\"{schema_destino}.modelos_registry\"))\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"INFORMA√á√ïES IMPORTANTES\")\n",
    "print(\"-\"*80)\n",
    "print(f\"\\nüÜî RUN_ID: {run_id}\")\n",
    "print(f\"üìÅ Volume: {volume_path}\")\n",
    "print(f\"\\n‚úÖ Para carregar os modelos no notebook de infer√™ncia:\")\n",
    "print(f\"   import mlflow\")\n",
    "print(f\"   import os\")\n",
    "print(f\"   os.environ['MLFLOW_DFS_TMP'] = '{volume_path}'\")\n",
    "print(f\"   modelo = mlflow.spark.load_model('runs:/{run_id}/modelo_geral', dfs_tmpdir='{volume_path}')\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PASSO 6: TESTE DE CARREGAMENTO\n",
    "# ==============================================================================\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"TESTE: Validando carregamento dos modelos...\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "try:\n",
    "    teste = mlflow.spark.load_model(\n",
    "        f\"runs:/{run_id}/modelo_final\",\n",
    "        dfs_tmpdir=volume_path\n",
    "    )\n",
    "    print(\"‚úÖ TESTE PASSOU! Os modelos podem ser carregados.\")\n",
    "    print(\"   Voc√™ pode prosseguir para o notebook de infer√™ncia.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå TESTE FALHOU: {e}\")\n",
    "    print(\"   ‚ö†Ô∏è Pode haver problemas no notebook de infer√™ncia.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ‚úÖ‚úÖ PROCESSO COMPLETO! ‚úÖ‚úÖ‚úÖ\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nüéØ Pr√≥ximo passo:\")\n",
    "print(\"   Execute o notebook '05_Diamond_Inference_Pipeline'\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_Diamond_Model_Training_and_Evaluation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
